{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30867731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 91.44%\n",
      "Accuracy on training data: 1.00\n",
      "Accuracy on test data:     0.91\n",
      "0.9570523912765954\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import string\n",
    "import re\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import emoji\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "from sklearn.metrics import roc_curve, auc, classification_report, balanced_accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, fbeta_score, recall_score, accuracy_score, f1_score, precision_score\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "tweets = pd.read_csv(\"Tweets.csv\")\n",
    "\n",
    "tweets = tweets.drop(columns=[\"airline_sentiment_gold\" , \"negativereason_gold\", \"tweet_coord\"])\n",
    "\n",
    "tweets = tweets.drop_duplicates(subset=[\"tweet_id\"], keep=False) \n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].str.lower()\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].str.replace('\\d+', '', regex=True)\n",
    "\n",
    "\n",
    "def remove_punc(text):\n",
    "    words_wo_punct = re.sub(r\"[^A-Za-z0-9\\s]+\", \"\", text)\n",
    "    return words_wo_punct\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: remove_punc(x))\n",
    "\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].str.strip()\n",
    "\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: emoji.demojize(x))\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    lem_text = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    return lem_text\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: word_lemmatizer(x))\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    english_stopwords = stopwords.words('english')\n",
    "    tokens_wo_stopwords = [t for t in tokens if t not in english_stopwords]\n",
    "    \n",
    "    return tokens_wo_stopwords\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: remove_stopwords(str(x)))\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].astype(str)\n",
    "    \n",
    "tweets = tweets[tweets[\"airline_sentiment\"] != \"neutral\"]\n",
    "\n",
    "def changeSentiment(sentiment):\n",
    "    if  sentiment == \"positive\":\n",
    "        return 0\n",
    "    elif sentiment == \"negative\":\n",
    "        return 1   \n",
    "tweets['airline_sentiment'] = tweets['airline_sentiment'].apply(lambda x : changeSentiment(x))\n",
    "\n",
    "ngram = (1,2)\n",
    "cv = CountVectorizer(ngram_range=ngram)\n",
    "X = cv.fit_transform(tweets['text'])\n",
    "y = tweets['airline_sentiment']\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=.30, random_state=0)\n",
    "\n",
    "\n",
    "lr2 = LogisticRegression(max_iter=1000, C=1).fit(X_train2, y_train2)\n",
    "\n",
    "### Print accuracy ###\n",
    "print(\"Logistic Regression Accuracy: %0.2f%%\" % (100 * lr2.score(X_test2, y_test2)))\n",
    "y_pred =lr2.predict(X_test2)\n",
    "training_accuracy = lr2.score(X_train2, y_train2)\n",
    "test_accuracy = lr2.score(X_test2, y_test2)\n",
    "\n",
    "print(\"Accuracy on training data: %0.2f\" % (training_accuracy))\n",
    "print(\"Accuracy on test data:     %0.2f\" % (test_accuracy))\n",
    "print(roc_auc_score( y_test2, lr2.predict_proba(X_test2)[:,1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afddbc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ad555b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 92.18%\n",
      "Accuracy on training data: 0.95\n",
      "Accuracy on test data:     0.92\n",
      "0.9577395058139224\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import string\n",
    "import re\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import emoji\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "from sklearn.metrics import roc_curve, auc, classification_report, balanced_accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, fbeta_score, recall_score, accuracy_score, f1_score, precision_score\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "tweets = pd.read_csv(\"Tweets.csv\")\n",
    "\n",
    "tweets = tweets.drop(columns=[\"airline_sentiment_gold\" , \"negativereason_gold\", \"tweet_coord\"])\n",
    "\n",
    "tweets = tweets.drop_duplicates(subset=[\"tweet_id\"], keep=False) \n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].str.lower()\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].str.replace('\\d+', '', regex=True)\n",
    "\n",
    "\n",
    "def remove_punc(text):\n",
    "    words_wo_punct = re.sub(r\"[^A-Za-z0-9\\s]+\", \"\", text)\n",
    "    return words_wo_punct\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: remove_punc(x))\n",
    "\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].str.strip()\n",
    "\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: emoji.demojize(x))\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    lem_text = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    return lem_text\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: word_lemmatizer(x))\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    english_stopwords = stopwords.words('english')\n",
    "    tokens_wo_stopwords = [t for t in tokens if t not in english_stopwords]\n",
    "    \n",
    "    return tokens_wo_stopwords\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: remove_stopwords(str(x)))\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].astype(str)\n",
    "    \n",
    "tweets = tweets[tweets[\"airline_sentiment\"] != \"neutral\"]\n",
    "\n",
    "def changeSentiment(sentiment):\n",
    "    if  sentiment == \"positive\":\n",
    "        return 0\n",
    "    elif sentiment == \"negative\":\n",
    "        return 1   \n",
    "tweets['airline_sentiment'] = tweets['airline_sentiment'].apply(lambda x : changeSentiment(x))\n",
    "cv = CountVectorizer(min_df=5, max_df=0.70)\n",
    "X = cv.fit_transform(tweets.text)\n",
    "y = tweets['airline_sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=0)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logReg = LogisticRegression(max_iter=1000).fit(X_train, y_train)\n",
    "\n",
    "### Print accuracy ###\n",
    "print(\"Logistic Regression Accuracy: %0.2f%%\" % (100 * logReg.score(X_test, y_test)))\n",
    "\n",
    "training_accuracy = logReg.score(X_train, y_train)\n",
    "test_accuracy = logReg.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy on training data: %0.2f\" % (training_accuracy))\n",
    "print(\"Accuracy on test data:     %0.2f\" % (test_accuracy))\n",
    "print(roc_auc_score( y_test, logReg.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec371496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 90.94%\n",
      "Accuracy on training data: 0.92\n",
      "Accuracy on test data:     0.91\n",
      "0.9583204152939573\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import string\n",
    "import re\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import emoji\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "from sklearn.metrics import roc_curve, auc, classification_report, balanced_accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, fbeta_score, recall_score, accuracy_score, f1_score, precision_score\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "tweets = pd.read_csv(\"Tweets.csv\")\n",
    "\n",
    "tweets = tweets.drop(columns=[\"airline_sentiment_gold\" , \"negativereason_gold\", \"tweet_coord\"])\n",
    "\n",
    "tweets = tweets.drop_duplicates(subset=[\"tweet_id\"], keep=False) \n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].str.lower()\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].str.replace('\\d+', '', regex=True)\n",
    "\n",
    "\n",
    "def remove_punc(text):\n",
    "    words_wo_punct = re.sub(r\"[^A-Za-z0-9\\s]+\", \"\", text)\n",
    "    return words_wo_punct\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: remove_punc(x))\n",
    "\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].str.strip()\n",
    "\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: emoji.demojize(x))\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    lem_text = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    return lem_text\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: word_lemmatizer(x))\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    english_stopwords = stopwords.words('english')\n",
    "    tokens_wo_stopwords = [t for t in tokens if t not in english_stopwords]\n",
    "    \n",
    "    return tokens_wo_stopwords\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: remove_stopwords(str(x)))\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].astype(str)\n",
    "    \n",
    "tweets = tweets[tweets[\"airline_sentiment\"] != \"neutral\"]\n",
    "\n",
    "def changeSentiment(sentiment):\n",
    "    if  sentiment == \"positive\":\n",
    "        return 0\n",
    "    elif sentiment == \"negative\":\n",
    "        return 1   \n",
    "tweets['airline_sentiment'] = tweets['airline_sentiment'].apply(lambda x : changeSentiment(x))\n",
    "\n",
    "td = TfidfVectorizer(min_df=5, max_df=0.70)\n",
    "X = td.fit_transform(tweets['text'])\n",
    "y = tweets['airline_sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=0)\n",
    "\n",
    "logReg2 = LogisticRegression(max_iter=1000).fit(X_train, y_train)\n",
    "\n",
    "### Print accuracy ###\n",
    "print(\"Logistic Regression Accuracy: %0.2f%%\" % (100 * logReg2.score(X_test, y_test)))\n",
    "\n",
    "training_accuracy = logReg2.score(X_train, y_train)\n",
    "test_accuracy = logReg2.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy on training data: %0.2f\" % (training_accuracy))\n",
    "print(\"Accuracy on test data:     %0.2f\" % (test_accuracy))\n",
    "print(roc_auc_score( y_test, logReg2.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efddb464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 89.17%\n",
      "Accuracy on training data: 0.91\n",
      "Accuracy on test data:     0.89\n",
      "0.9511944850610249\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import string\n",
    "import re\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import emoji\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "from sklearn.metrics import roc_curve, auc, classification_report, balanced_accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, fbeta_score, recall_score, accuracy_score, f1_score, precision_score\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "tweets = pd.read_csv(\"Tweets.csv\")\n",
    "\n",
    "tweets = tweets.drop(columns=[\"airline_sentiment_gold\" , \"negativereason_gold\", \"tweet_coord\"])\n",
    "\n",
    "tweets = tweets.drop_duplicates(subset=[\"tweet_id\"], keep=False) \n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].str.lower()\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].str.replace('\\d+', '', regex=True)\n",
    "\n",
    "\n",
    "def remove_punc(text):\n",
    "    words_wo_punct = re.sub(r\"[^A-Za-z0-9\\s]+\", \"\", text)\n",
    "    return words_wo_punct\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: remove_punc(x))\n",
    "\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].str.strip()\n",
    "\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: emoji.demojize(x))\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    lem_text = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    return lem_text\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: word_lemmatizer(x))\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    english_stopwords = stopwords.words('english')\n",
    "    tokens_wo_stopwords = [t for t in tokens if t not in english_stopwords]\n",
    "    \n",
    "    return tokens_wo_stopwords\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: remove_stopwords(str(x)))\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].astype(str)\n",
    "    \n",
    "tweets = tweets[tweets[\"airline_sentiment\"] != \"neutral\"]\n",
    "\n",
    "def changeSentiment(sentiment):\n",
    "    if  sentiment == \"positive\":\n",
    "        return 0\n",
    "    elif sentiment == \"negative\":\n",
    "        return 1   \n",
    "tweets['airline_sentiment'] = tweets['airline_sentiment'].apply(lambda x : changeSentiment(x))\n",
    "tf = TfidfVectorizer(min_df=5, max_df=0.70)\n",
    "X = tf.fit_transform(tweets['text'])\n",
    "y = tweets['airline_sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=0)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "### Print accuracy ###\n",
    "print(\"Naive Bayes Accuracy: %0.2f%%\" % (100 * mnb.score(X_test, y_test)))\n",
    "\n",
    "training_accuracy = mnb.score(X_train, y_train)\n",
    "test_accuracy = mnb.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy on training data: %0.2f\" % (training_accuracy))\n",
    "print(\"Accuracy on test data:     %0.2f\" % (test_accuracy))\n",
    "print(roc_auc_score( y_test, mnb.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48885d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 89.76%\n",
      "Accuracy on training data: 1.00\n",
      "Accuracy on test data:     0.90\n",
      "0.9548698237103328\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import string\n",
    "import re\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import emoji\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "from sklearn.metrics import roc_curve, auc, classification_report, balanced_accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, fbeta_score, recall_score, accuracy_score, f1_score, precision_score\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "tweets = pd.read_csv(\"Tweets.csv\")\n",
    "\n",
    "tweets = tweets.drop(columns=[\"airline_sentiment_gold\" , \"negativereason_gold\", \"tweet_coord\"])\n",
    "\n",
    "tweets = tweets.drop_duplicates(subset=[\"tweet_id\"], keep=False) \n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].str.lower()\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].str.replace('\\d+', '', regex=True)\n",
    "\n",
    "\n",
    "def remove_punc(text):\n",
    "    words_wo_punct = re.sub(r\"[^A-Za-z0-9\\s]+\", \"\", text)\n",
    "    return words_wo_punct\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: remove_punc(x))\n",
    "\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].str.strip()\n",
    "\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: emoji.demojize(x))\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    lem_text = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    return lem_text\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: word_lemmatizer(x))\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    english_stopwords = stopwords.words('english')\n",
    "    tokens_wo_stopwords = [t for t in tokens if t not in english_stopwords]\n",
    "    \n",
    "    return tokens_wo_stopwords\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: remove_stopwords(str(x)))\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].astype(str)\n",
    "    \n",
    "tweets = tweets[tweets[\"airline_sentiment\"] != \"neutral\"]\n",
    "\n",
    "def changeSentiment(sentiment):\n",
    "    if  sentiment == \"positive\":\n",
    "        return 0\n",
    "    elif sentiment == \"negative\":\n",
    "        return 1   \n",
    "tweets['airline_sentiment'] = tweets['airline_sentiment'].apply(lambda x : changeSentiment(x))\n",
    "tf = TfidfVectorizer(min_df=5, max_df=0.70)\n",
    "X = tf.fit_transform(tweets['text'])\n",
    "y = tweets['airline_sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=0)\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier().fit(X_train, y_train)\n",
    "\n",
    "### Print accuracy ###\n",
    "print(\"Random Forest Accuracy: %0.2f%%\" % (100 * rf.score(X_test, y_test)))\n",
    "\n",
    "training_accuracy = rf.score(X_train, y_train)\n",
    "test_accuracy = rf.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy on training data: %0.2f\" % (training_accuracy))\n",
    "print(\"Accuracy on test data:     %0.2f\" % (test_accuracy))\n",
    "print(roc_auc_score( y_test, logReg.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "990c1f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 90.94%\n",
      "Accuracy on training data: 0.92\n",
      "Accuracy on test data:     0.91\n",
      "0.9583204152939573\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import string\n",
    "import re\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import emoji\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "from sklearn.metrics import roc_curve, auc, classification_report, balanced_accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, fbeta_score, recall_score, accuracy_score, f1_score, precision_score\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "tweets = pd.read_csv(\"Tweets.csv\")\n",
    "\n",
    "tweets = tweets.drop(columns=[\"airline_sentiment_gold\" , \"negativereason_gold\", \"tweet_coord\"])\n",
    "\n",
    "tweets = tweets.drop_duplicates(subset=[\"tweet_id\"], keep=False) \n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].str.lower()\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].str.replace('\\d+', '', regex=True)\n",
    "\n",
    "\n",
    "def remove_punc(text):\n",
    "    words_wo_punct = re.sub(r\"[^A-Za-z0-9\\s]+\", \"\", text)\n",
    "    return words_wo_punct\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: remove_punc(x))\n",
    "\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].str.strip()\n",
    "\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: emoji.demojize(x))\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    lem_text = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    return lem_text\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: word_lemmatizer(x))\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    english_stopwords = stopwords.words('english')\n",
    "    tokens_wo_stopwords = [t for t in tokens if t not in english_stopwords]\n",
    "    \n",
    "    return tokens_wo_stopwords\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: remove_stopwords(str(x)))\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].astype(str)\n",
    "    \n",
    "tweets = tweets[tweets[\"airline_sentiment\"] != \"neutral\"]\n",
    "\n",
    "def changeSentiment(sentiment):\n",
    "    if  sentiment == \"positive\":\n",
    "        return 0\n",
    "    elif sentiment == \"negative\":\n",
    "        return 1   \n",
    "tweets['airline_sentiment'] = tweets['airline_sentiment'].apply(lambda x : changeSentiment(x))\n",
    "tf = TfidfVectorizer(min_df=5, max_df=0.70)\n",
    "X = tf.fit_transform(tweets['text'])\n",
    "y = tweets['airline_sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=0)\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000).fit(X_train, y_train)\n",
    "\n",
    "print(\"Logistic Regression Accuracy: %0.2f%%\" % (100 * lr.score(X_test, y_test)))\n",
    "\n",
    "training_accuracy = lr.score(X_train, y_train)\n",
    "test_accuracy = lr.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy on training data: %0.2f\" % (training_accuracy))\n",
    "print(\"Accuracy on test data:     %0.2f\" % (test_accuracy))\n",
    "print(roc_auc_score( y_test, lr.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a024febd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'max_iter': 1000}\n",
      "0.9499753030090758\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'C' : [.01, .001, 1, 10, 100],\n",
    "    'max_iter': [1000, 10000]\n",
    "}\n",
    "\n",
    "lr_gs = GridSearchCV(lr, parameters, cv=5, scoring = \"roc_auc\") \n",
    "lr_gs.fit(X_train, y_train)\n",
    "\n",
    "print(lr_gs.best_params_)\n",
    "print(lr_gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "374bce9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 90.94%\n",
      "Accuracy on training data: 0.92\n",
      "Accuracy on test data:     0.91\n",
      "0.9583204152939573\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=1000, C=1).fit(X_train, y_train)\n",
    "\n",
    "print(\"Logistic Regression Accuracy: %0.2f%%\" % (100 * lr.score(X_test, y_test)))\n",
    "\n",
    "training_accuracy = lr.score(X_train, y_train)\n",
    "test_accuracy = lr.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy on training data: %0.2f\" % (training_accuracy))\n",
    "print(\"Accuracy on test data:     %0.2f\" % (test_accuracy))\n",
    "print(roc_auc_score( y_test, lr.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65778672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 91.44%\n",
      "Accuracy on training data: 1.00\n",
      "Accuracy on test data:     0.91\n",
      "0.9570523912765954\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import string\n",
    "import re\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import emoji\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "from sklearn.metrics import roc_curve, auc, classification_report, balanced_accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, fbeta_score, recall_score, accuracy_score, f1_score, precision_score\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "tweets = pd.read_csv(\"Tweets.csv\")\n",
    "\n",
    "tweets = tweets.drop(columns=[\"airline_sentiment_gold\" , \"negativereason_gold\", \"tweet_coord\"])\n",
    "\n",
    "tweets = tweets.drop_duplicates(subset=[\"tweet_id\"], keep=False) \n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].str.lower()\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].str.replace('\\d+', '', regex=True)\n",
    "\n",
    "\n",
    "def remove_punc(text):\n",
    "    words_wo_punct = re.sub(r\"[^A-Za-z0-9\\s]+\", \"\", text)\n",
    "    return words_wo_punct\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: remove_punc(x))\n",
    "\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].str.strip()\n",
    "\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: emoji.demojize(x))\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    lem_text = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    return lem_text\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: word_lemmatizer(x))\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    english_stopwords = stopwords.words('english')\n",
    "    tokens_wo_stopwords = [t for t in tokens if t not in english_stopwords]\n",
    "    \n",
    "    return tokens_wo_stopwords\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: remove_stopwords(str(x)))\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].astype(str)\n",
    "    \n",
    "tweets = tweets[tweets[\"airline_sentiment\"] != \"neutral\"]\n",
    "\n",
    "def changeSentiment(sentiment):\n",
    "    if  sentiment == \"positive\":\n",
    "        return 0\n",
    "    elif sentiment == \"negative\":\n",
    "        return 1   \n",
    "tweets['airline_sentiment'] = tweets['airline_sentiment'].apply(lambda x : changeSentiment(x))\n",
    "\n",
    "ngram = (1,2)\n",
    "cv = CountVectorizer(ngram_range=ngram)\n",
    "X = cv.fit_transform(tweets['text'])\n",
    "y = tweets['airline_sentiment']\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=.30, random_state=0)\n",
    "\n",
    "lr2 = LogisticRegression(max_iter=1000, C=1).fit(X_train2, y_train2)\n",
    "\n",
    "### Print accuracy ###\n",
    "print(\"Logistic Regression Accuracy: %0.2f%%\" % (100 * lr2.score(X_test2, y_test2)))\n",
    "y_pred =lr2.predict(X_test2)\n",
    "training_accuracy = lr2.score(X_train2, y_train2)\n",
    "test_accuracy = lr2.score(X_test2, y_test2)\n",
    "\n",
    "print(\"Accuracy on training data: %0.2f\" % (training_accuracy))\n",
    "print(\"Accuracy on test data:     %0.2f\" % (test_accuracy))\n",
    "print(roc_auc_score( y_test2, lr2.predict_proba(X_test2)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5af9bd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 87.72%\n",
      "Accuracy on training data: 0.91\n",
      "Accuracy on test data:     0.88\n",
      "0.9517174645098101\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import string\n",
    "import re\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import emoji\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "from sklearn.metrics import roc_curve, auc, classification_report, balanced_accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, fbeta_score, recall_score, accuracy_score, f1_score, precision_score\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "tweets = pd.read_csv(\"Tweets.csv\")\n",
    "\n",
    "tweets = tweets.drop(columns=[\"airline_sentiment_gold\" , \"negativereason_gold\", \"tweet_coord\"])\n",
    "\n",
    "tweets = tweets.drop_duplicates(subset=[\"tweet_id\"], keep=False) \n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].str.lower()\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].str.replace('\\d+', '', regex=True)\n",
    "\n",
    "\n",
    "def remove_punc(text):\n",
    "    words_wo_punct = re.sub(r\"[^A-Za-z0-9\\s]+\", \"\", text)\n",
    "    return words_wo_punct\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: remove_punc(x))\n",
    "\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].str.strip()\n",
    "\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: emoji.demojize(x))\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    lem_text = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    return lem_text\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: word_lemmatizer(x))\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    english_stopwords = stopwords.words('english')\n",
    "    tokens_wo_stopwords = [t for t in tokens if t not in english_stopwords]\n",
    "    \n",
    "    return tokens_wo_stopwords\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: remove_stopwords(str(x)))\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].astype(str)\n",
    "    \n",
    "tweets = tweets[tweets[\"airline_sentiment\"] != \"neutral\"]\n",
    "\n",
    "def changeSentiment(sentiment):\n",
    "    if  sentiment == \"positive\":\n",
    "        return 0\n",
    "    elif sentiment == \"negative\":\n",
    "        return 1   \n",
    "tweets['airline_sentiment'] = tweets['airline_sentiment'].apply(lambda x : changeSentiment(x))\n",
    "\n",
    "ngram = (1,2)\n",
    "td = TfidfVectorizer(ngram_range=ngram)\n",
    "X = td.fit_transform(tweets['text'])\n",
    "y = tweets['airline_sentiment']\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=.30, random_state=0)\n",
    "\n",
    "lr3 = LogisticRegression(max_iter=1000, C=1).fit(X_train3, y_train3)\n",
    "\n",
    "### Print accuracy ###\n",
    "print(\"Logistic Regression Accuracy: %0.2f%%\" % (100 * lr3.score(X_test3, y_test3)))\n",
    "y_pred =lr3.predict(X_test3)\n",
    "training_accuracy = lr3.score(X_train3, y_train3)\n",
    "test_accuracy = lr3.score(X_test3, y_test3)\n",
    "\n",
    "print(\"Accuracy on training data: %0.2f\" % (training_accuracy))\n",
    "print(\"Accuracy on test data:     %0.2f\" % (test_accuracy))\n",
    "print(roc_auc_score(y_test3, lr3.predict_proba(X_test3)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4511a6ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3dbe4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Training Accuracy: 99.77%\n",
      "Logistic Regression Test Accuracy: 91.50%\n",
      "ROC AUC Score: 0.9569585231704033\n",
      "Predicted Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import string\n",
    "import re\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import emoji\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Load the dataset\n",
    "tweets = pd.read_csv(\"Tweets.csv\")\n",
    "\n",
    "# Drop unnecessary columns and duplicates\n",
    "tweets = tweets.drop(columns=[\"airline_sentiment_gold\", \"negativereason_gold\", \"tweet_coord\"])\n",
    "tweets = tweets.drop_duplicates(subset=[\"tweet_id\"], keep=False)\n",
    "\n",
    "# Preprocessing functions\n",
    "def remove_punc(text):\n",
    "    words_wo_punct = re.sub(r\"[^A-Za-z0-9\\s]+\", \"\", text)\n",
    "    return words_wo_punct\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    lem_text = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    return lem_text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    english_stopwords = stopwords.words('english')\n",
    "    tokens_wo_stopwords = [t for t in tokens if t not in english_stopwords]\n",
    "    return ' '.join(tokens_wo_stopwords)\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = remove_punc(tweet)\n",
    "    tweet = emoji.demojize(tweet)\n",
    "    tweet = word_lemmatizer(tweet)\n",
    "    tweet = remove_stopwords(tweet)\n",
    "    return tweet\n",
    "\n",
    "# Apply preprocessing\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(preprocess_tweet)\n",
    "\n",
    "# Remove neutral sentiment\n",
    "tweets = tweets[tweets[\"airline_sentiment\"] != \"neutral\"]\n",
    "\n",
    "# Map sentiment to numerical values\n",
    "def change_sentiment(sentiment):\n",
    "    if sentiment == \"positive\":\n",
    "        return 0\n",
    "    elif sentiment == \"negative\":\n",
    "        return 1\n",
    "\n",
    "tweets['airline_sentiment'] = tweets['airline_sentiment'].apply(change_sentiment)\n",
    "\n",
    "# Train-test split\n",
    "X = tweets['text']\n",
    "y = tweets['airline_sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=0)\n",
    "\n",
    "# Vectorize text data\n",
    "cv = CountVectorizer(ngram_range=(1, 2))\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "X_test_cv = cv.transform(X_test)\n",
    "\n",
    "# Train logistic regression model\n",
    "lr = LogisticRegression(max_iter=1000, C=1)\n",
    "lr.fit(X_train_cv, y_train)\n",
    "\n",
    "# Print accuracy\n",
    "train_accuracy = lr.score(X_train_cv, y_train)\n",
    "test_accuracy = lr.score(X_test_cv, y_test)\n",
    "print(\"Logistic Regression Training Accuracy: {:.2f}%\".format(train_accuracy * 100))\n",
    "print(\"Logistic Regression Test Accuracy: {:.2f}%\".format(test_accuracy * 100))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test, lr.predict_proba(X_test_cv)[:, 1]))\n",
    "\n",
    "# Function to classify a tweet\n",
    "def classify_tweet(tweet):\n",
    "    preprocessed_tweet = preprocess_tweet(tweet)\n",
    "    vectorized_tweet = cv.transform([preprocessed_tweet])\n",
    "    prediction = lr.predict(vectorized_tweet)\n",
    "    return \"Positive\" if prediction == 0 else \"Negative\"\n",
    "\n",
    "# Example usage\n",
    "input_tweet = \"i love this flights and crew was also very kind to me.\"\n",
    "predicted_sentiment = classify_tweet(input_tweet)\n",
    "print(\"Predicted Sentiment:\", predicted_sentiment) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea0b11d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
